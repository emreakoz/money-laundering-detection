{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Money Laudering Detection Through Social Network Analysis\n",
    "\n",
    "This notebook discusses the sophisticated layering strategies used by criminals to cover money laundering schemes and possible ways to detect the association between the sender and the receiver. This problem is posed as an interview question and the statement can be summarized as follows,\n",
    "\n",
    "Money laundering (ML) is the process of transforming the profits of crime and corruption into legitimate assets. To this end, criminal organizations and entities use the legal banking system to ingest money acquired from illegal activities, which is later withdrawn as legitimate payments.\n",
    "\n",
    "One ML technique used by criminals is the so-called \"bridging\", where an entity A sends money to an entity B via an entity C (i.e., A->C->B), creating a layer of dissociation between entities A and B. In one instance, a drug dealer 'John Smith' may want to pay his immediate drug distributor 'Mary Jerry'. They may then agree to use Mary's brother-in-law 'Joseph Louis' to intermediate the transaction, serving as a \"bridge\" between John and Mary. Joseph therefore will start forwarding every payment made by John to Mary and, depending on the agreement, may take a percentage of the payments as fees for his \"services\". Purpose of the current notebook is to scan financial records in search of bridging transactions and entities suspected of bridging and finally sort the transactions based on the suspicion level from high to low.\n",
    "\n",
    "The data for this exercise has an unique identifier, a timestamp, the transferred amount, the sender's account id and the receiver's account id. Given that the data does not have labels, I analyzed the transactions as a graph network and investigate the red flags between transactions. My approach can be summarized as follows,\n",
    "* I grouped the transactions based on a variable timeframe. This variable divides all the data set into smaller timeframes and the money laundering algorithm is run on each timeframe separately (The timeframe is set to 1 day for this problem).\n",
    "* For each given timeframe, look at the total money transfer from each account. If the transfer from an account is below a cut off value, I assumed that the money transfer is 'not worth' tracking. (Again this cut off value is a variable and it can be tuned based on the sensitivity of the analysis. I set it to \\$10000, if one wants to be more conservative, this value can be decreased.)\n",
    "* For the remaining accounts, I constructed a graph network where the entities are the vertices and the transferred money is the edges connecting these vertices. Just to be clear, each day has its own network.\n",
    "* Looking at these networks, if the money transfer chain has fewer than two edges within a given timeframe, that means a bridging account does not exist so I got rid of these transactions.\n",
    "* To separate the 'clean' transaction chains from the possibly money laundering networks, I applied one last layer of filtering. If at any point in the transfer chain, the ratio of the money received by an account to the money leaving the initial sender is above a threshold value ($\\alpha_{receiver}/\\alpha_{sender} > 0.9$), I kept these groups as they are highly suspicious. (The threshold value is a variable and it is assumed to be 0.9 for this discussion.)\n",
    "* Finally, I sorted the highly suspicious accounts and transfers by measuring certain features of the networks, such as; degree centrality, closeness centrality and betweenness centrality. \n",
    "\n",
    "This filtering approach significantly reduces the number of the transactions to a manageable subset. Two common topologies of money laundering networks is shown below. Network (a) has one sender account and one reciever account with many intermediary accounts. These transfers are modelled by directed edges between nodes. The funds are divided by the sender among intermediary accounts, which then resend the funds to the receiver account. On the other hand, network (b) has one sender and one receiver and a row of intermediary accounts. The intermediary accounts pass the entire amount (minus the optional commision) of funds to the next account until ultimately they reach to the receiver account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./Figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common networks described above may not generalize to sophisticated tansaction chains. An example of a more generic money laundering network is shown in the figure below which summarizes the idea behind the algorithm in this notebook. I wanted to motivate this model by showing a few important points on the graph. Here, node A sends money to node Z via nodes B, C, E and D. The first important point is that the transfer is divided into chunks and sent through different bridging nodes. Additionally, a node does not have to send the money as a one time transfer. The total amount can be divided into fractions and can be sent different times of the day, as shown by the edges in between nodes E and Z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](./Figure2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the filtering analysis is as follows. For cleanliness, I kept the helper functions out of the discussion, they can be reached as a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Before the Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import networkx as nx\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing the network, initial filtering was suitable for distributed computing so I took advantage of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSACTION|TIMESTAMP|AMOUNT|SENDER|RECEIVER\n",
      "110cbb2d21e3858290cdc14ac7d50fbdf84a67ba0d71c79729d5ebd2eea93100|2005-12-07|1487.00|ID00001693732070|ID00001693757810\n"
     ]
    }
   ],
   "source": [
    "money_cut_off = 10000.\n",
    "commission_cut_off = 0.9\n",
    "group_by_time = 1 #days\n",
    "\n",
    "sc = SparkContext(appName = \"graph\")\n",
    "lines = sc.textFile('./transactions.csv')\n",
    "header = lines.first()\n",
    "lines = lines.filter(lambda x: x != header)\n",
    "lines = lines.filter(lambda x: x.split('|')[1] != '2006-02-29')\n",
    "print(header, lines.first(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prescribed the cut off variables and timeframe as described above. The data did not require almost any cleaning or imputation. The only problem was that there were transactions recorded on 02-29-2006, which is not an actual date, so I had to remove those transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_by_amount_per_node = get_amount_per_node_transfer(lines)\n",
    "graph_edge_constructor, filtered_transactions = get_graph_edges(lines, filtered_by_amount_per_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I grouped the transactions by the date and the sender account to understand the total sent money from each account per given timeframe. The timeframe can be tuned by increments of a day as I converted the timestamp into the proleptic Gregorian date. Then, I distinguished and removed the transactions involving accounts that transfer below $\\$$10000 for a given day. I did not do this money filtering on each transaction because as described previously some accounts might divide the cut off value ($\\$$10000) into multiple chunks and send as batches. The total amount of money transfer per day per node is stored in a dictionary after the filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_normalized_transactions, transaction_volume, betweenness_dict, closeness_dict = [], {}, {}, {}\n",
    "###main function\n",
    "for date, all_transactions_per_time in zip(graph_edge_constructor.keys(), graph_edge_constructor.values()):\n",
    "    #Creates different graphs for different days\n",
    "    G = create_graph([trans for trans in all_transactions_per_time])\n",
    "    max_subgraph_size, visited_subgraphs = 0, []\n",
    "    \n",
    "    for transaction in all_transactions_per_time:\n",
    "        # The depth of the transaction chain starting from the inital sender\n",
    "        depth = get_depth(G, transaction[0])\n",
    "        \n",
    "        if depth > 1:\n",
    "            #get connected nodes in bfs order\n",
    "            connected_nodes = breadth_first_search(G, transaction[0])\n",
    "            \n",
    "            if max_subgraph_size  < len(connected_nodes) or not set(connected_nodes) <= set(visited_subgraphs):\n",
    "                max_subgraph_size = len(connected_nodes)\n",
    "                visited_subgraphs += connected_nodes\n",
    "                # I looked at the closeness and betweenness for only \n",
    "                # transaction groups with multiple edges.\n",
    "                # This saved significant amount of computational time \n",
    "                # as graph size decreases once the \n",
    "                # transactions with single edges are filtered.\n",
    "                sub_G = G.subgraph(connected_nodes)\n",
    "                betweenness_nodes = nx.betweenness_centrality(sub_G, normalized=True)\n",
    "                closeness_nodes = nx.closeness_centrality(sub_G)\n",
    "                \n",
    "                # Edge weights are normalized by the money initially sent from the sender.\n",
    "                # Also in and out degree weights are calculated for each node.\n",
    "                normalized_transactions, transaction_volume = get_normalized_edge_weights(G, \n",
    "                                date, connected_nodes, transaction_volume, filtered_by_amount_per_node)\n",
    "                \n",
    "                if normalized_transactions not in all_normalized_transactions:\n",
    "                    all_normalized_transactions.append(normalized_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph analysis involves the following methodology,\n",
    "* Create a graph for each day. Meaning, the money transaction chain for each day is separately analyzed. Correlations over time is not considered.\n",
    "* Starting from a node, get all the connected nodes with bread first search order.\n",
    "* Construct subgraphs by using these subgroups of connected nodes to calculate betweenness and closeness.\n",
    "* Similar to the closeness and betweenness, a parameter is described which measures the frequency of money flow over the nodes (I will call it transaction traffic parameter for the rest of the notebook). It is defined as the total number of inbound and outbound transfers per node on a given day normalized by the maximum number of inbound and outbound transfers on that day which maps the parameter into [0,1] range.\n",
    "* Normalize each edge weight in the subgraphs by the total money initially sent from the first node. This will let us distinguish the accounts in the network which receives 100\\% (or close to 100\\%) of the money sent from the initial sender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix multiple edge receiver problem\n",
    "suspicious_transactions = get_suspicious_transactions(normalized_transactions, commission_cut_off)\n",
    "       \n",
    "#normalize the transaction traffic parameter\n",
    "suspicious_traffic = get_suspicious_traffic(transaction_volume) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some further manipulation is needed in case multiple edges are pointing to the same receiver node. Multiple transactions from a single source to the same receiver node strategy might be used by the criminals to confuse financial crime investigators. If an edge in the money transfer chain has a weight above a threshold value (in this case I set it to 0.9) and below 1, that means 90-100% of the initially sent money is received by a connected node. The 10% gap in the ratio is set according to the problem description. These group of transactions are separated as highly suspicious transaction chains which are to be further manipulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Assesment\n",
    "\n",
    "The transactions and the entities are ranked based on how suspicious they are, according to the following methodology,\n",
    "\n",
    "* A gamma parameter ($\\gamma$) is calculated per node as a linear combination of the node's traffic, betweenness and closeness parameters. There was not any labeled data to tune the weights of these parameters so I referred mostly to [2] while deciding on the weights. A labelled data set would be helpful in the case of a real application to cross validate and change these weights.\n",
    "* The highest contribution to the gamma parameteer comes from the traffic paramter, then betweenness parameter and a small contribution from the closeness parameter.\n",
    "* The gamma parameter describes the 'suspicion' of a given node. The higher the gamma parameter, the higher the risk that the given node is involved in a money laundering scheme at a given day. An important detail here is that, although I analyzed each day on its own during the graph analysis, for the final gamma parameter calculation, I looked at the total value of the gamma values accumulated over time. Therefore, a node which has a very high gamma parameter for one day might be less suspicious than another node which has smaller gamma parameters over a long timeframe. This takes into account the historical activity of each node.\n",
    "* For each given transaction chain, the entities involved in the money transfer, determines the total 'suspicion' of that transfer chain. Thus, graphs involving highly risky entities has the highest suspicion.\n",
    "\n",
    "A small note here is that; I did not look at the individual transactions, rather I considered a full chain of transactions when I analyze the suspicion level of the transactions. Therefore, each chain has the same gamma parameter and consequently the same suspicion level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gamma parameters calculated for each node and sorted from high to low\n",
    "gamma_nodes, gamma_dict = get_gamma(suspicious_traffic, betweenness_dict, closeness_dict)\n",
    "\n",
    "#gamma parameters calculated for each transaction chain\n",
    "suspicious_transactions_by_line = get_suspicious_transactions(suspicious_transactions, gamma_nodes)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gamma parameter here is a linear combination of a few network features and as the data set does not have any labels, the coefficient of these features are based on literature and not to this specific problem. It would be nice to have a set of labelled data where I can understand the correlation of gamma parameter with money laundering activity. As it is not the case, I reported all the possible entities and transactions involved in the chain activities from high to low risk (aka high to low gamma). Labelling all these accounts and transactions as money laundering activity might result in a high number of false positives. To prevent this, a threshold gamma might be chosen where the transactions above it would be flagged as money laundering and vice versa.\n",
    "\n",
    "\n",
    "One last point is that, although I tried to model the problem as general as possible, the money laundering schemes can be highly sophisticated. Learning from the data might be superior to the rule-based approaches. An unsupervised graph learning model such as; DeepWalk might be useful to learn the social relations of the nodes in a graph model. These relations can be further analyzed to detect anomalies. However, given the timeframe and the computational constraints, I decided to not pursue this way when I attempted this specific problem.\n",
    "\n",
    "For further reference, this analysis is highly inspired by these two papers:\n",
    "\n",
    "[1] Soltani, Reza, et al. \"A new algorithm for money laundering detection based on structural similarity.\" 2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference.\n",
    "\n",
    "[2] Colladon, Andrea Fronzetti, and Elisa Remondi. \"Using social network analysis to prevent money laundering.\" 2017 Expert Systems with Applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
